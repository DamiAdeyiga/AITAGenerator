{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8c3c85-0ec4-4296-b014-22826320cbf6",
   "metadata": {
    "id": "6a8c3c85-0ec4-4296-b014-22826320cbf6"
   },
   "source": [
    "lstm based text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e639b269-bba8-448f-b1bc-8b1a29ff42cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e639b269-bba8-448f-b1bc-8b1a29ff42cc",
    "outputId": "3a409a4f-7a39-472b-bdbf-f547f89a2dda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\pablo\\anaconda3\\lib\\site-packages (22.3.1)\n",
      "Collecting pip\n",
      "  Using cached pip-23.1.2-py3-none-any.whl (2.1 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\Pablo\\anaconda3\\python.exe -m pip install --upgrade pip\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\pablo\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\pablo\\appdata\\roaming\\python\\python38\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from gensim) (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\pablo\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from gym) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from gym) (2.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: gym==0.21 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from stable-baselines3) (0.21.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from stable-baselines3) (2.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from stable-baselines3) (3.6.2)\n",
      "Requirement already satisfied: torch>=1.11 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from stable-baselines3) (1.13.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from stable-baselines3) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata~=4.13 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from stable-baselines3) (4.13.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from stable-baselines3) (1.23.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from importlib-metadata~=4.13->stable-baselines3) (3.11.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3) (4.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3) (1.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3) (22.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3) (9.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: language_tool_python in c:\\users\\pablo\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from language_tool_python) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from language_tool_python) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from requests->language_tool_python) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from requests->language_tool_python) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from requests->language_tool_python) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from requests->language_tool_python) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from tqdm->language_tool_python) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\pablo\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install gensim\n",
    "!pip install gym\n",
    "!pip install stable-baselines3\n",
    "!pip install language_tool_python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t9yaP0Mfya2k",
   "metadata": {
    "id": "t9yaP0Mfya2k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rCWXosXnybF9",
   "metadata": {
    "id": "rCWXosXnybF9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a473e1-990a-452e-bd1b-09d3285f7b02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71a473e1-990a-452e-bd1b-09d3285f7b02",
    "outputId": "67f2e866-f4e6-4ffe-e919-589f03903ee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: language_tool_python in /usr/local/lib/python3.10/dist-packages (2.7.1)\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
      "Collecting stable_baselines3\n",
      "  Using cached stable_baselines3-1.8.0-py3-none-any.whl (174 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (2.27.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
      "Collecting gym\n",
      "  Using cached gym-0.21.0.tar.gz (1.5 MB)\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy tensorflow keras nltk language_tool_python gym stable_baselines3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbf7b089-d0ee-479b-9a7f-90476d4b16b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "cbf7b089-d0ee-479b-9a7f-90476d4b16b9",
    "outputId": "812a6d2f-7c98-4988-8111-0cce908223c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  pkg_resources.declare_namespace(__name__)\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2349: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(parent)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-40ad11a11a86>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlanguage_tool_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEvalCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines3'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "import gym\n",
    "from gym import spaces\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import language_tool_python\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('aita_clean.csv', low_memory=True)\n",
    "df = df.dropna()\n",
    "# Filter the data for each class\n",
    "not_asshole = df[df['is_asshole'] == 0]\n",
    "asshole = df[df['is_asshole'] == 1]\n",
    "\n",
    "# Sample an equal number of instances from each class\n",
    "sample_size = 25000\n",
    "not_asshole_sample = not_asshole.sample(sample_size, random_state=42)\n",
    "asshole_sample = asshole.sample(sample_size, random_state=42)\n",
    "\n",
    "# Combine the sampled data and shuffle it\n",
    "df = pd.concat([not_asshole_sample, asshole_sample]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "max_title_len = 20\n",
    "max_body_len = 200\n",
    "input_len = max_title_len + max_body_len\n",
    "\n",
    "all_text = ' '.join(df['title'] + df['body'])\n",
    "unique_words = set(all_text)\n",
    "total_words = len(unique_words)\n",
    "\n",
    "#separator_token = \"<SEP>\"\n",
    "#texts = (df['title'] + separator_token + df['body']).tolist()\n",
    "texts = (df['title'] + ' ' + df['body']).tolist()\n",
    "tokenizer = Tokenizer(char_level=False)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "#tokenizer.word_index[separator_token] = len(tokenizer.word_index) + 1\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "sequences = []\n",
    "for text in texts:\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "\n",
    "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "input_sequences, target_sequences = sequences[:, :-1], sequences[:, -1]\n",
    "\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=input_len, padding='pre')\n",
    "target_sequences = to_categorical(target_sequences, num_classes=total_words)  # Convert to categorical format\n",
    "\n",
    "\n",
    "# Load GloVe word embeddings and create an Embeddings Dictionary\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('a2_glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "# Create Embedding Matrix having 100 columns\n",
    "embedding_matrix = zeros((total_words, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "embedding_matrix.shape\n",
    "\n",
    "#optimisation for slow laptop\n",
    "#1)sample_size = 100\n",
    "#2)max_sequence_len = 50  # for laptop sake since its slowwer , its really just fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f5ac3f-1894-4b55-8601-7f879406ac90",
   "metadata": {
    "id": "17f5ac3f-1894-4b55-8601-7f879406ac90"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, weights=[embedding_matrix], input_length=input_len, trainable=False, batch_input_shape=(1, input_len)))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, stateful=True))) # You can increase the number of LSTM units if you want\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, stateful=True)))\n",
    "model.add(TimeDistributed(Dense(total_words, activation='softmax')))\n",
    "\n",
    "\n",
    "# Compile the model with sparse_categorical_crossentropy\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train the LSTM model for text generation\n",
    "model.fit(input_sequences, target_sequences, epochs=100, verbose=1, batch_size=1) # Set batch_size=1 to maintain state between batches /should be at least 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c984fb3f-24ab-4d01-bb60-9b8db83418f6",
   "metadata": {
    "id": "c984fb3f-24ab-4d01-bb60-9b8db83418f6"
   },
   "outputs": [],
   "source": [
    "model.fit(input_sequences, target_sequences, epochs=1, verbose=1, batch_size=1) # Set batch_size=1 to maintain state between batches /should be at least 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbfd9a4-2099-40cc-9818-3d4da5965c05",
   "metadata": {
    "id": "fcbfd9a4-2099-40cc-9818-3d4da5965c05"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import re\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len, input_len, tokenizer, total_words):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=input_len, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "\n",
    "        # Get the probabilities corresponding to the words in the tokenizer\n",
    "        filtered_probs = predicted_probs.ravel()[:total_words]\n",
    "\n",
    "        # Sample a word from the filtered predicted probabilities\n",
    "        predicted_word_index = random.choices(range(total_words), weights=filtered_probs)[0]\n",
    "        # Get the corresponding word from the tokenizer\n",
    "        output_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_word_index:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    # Split the generated text into title and body\n",
    "    title_body_split = re.split(r'[.?!]', seed_text, 1)\n",
    "    title = title_body_split[0]\n",
    "    body = title_body_split[1] if len(title_body_split) > 1 else \"\"\n",
    "\n",
    "    return title, body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd2fff-f53b-4c7a-81ec-7aaf24984c97",
   "metadata": {
    "id": "b5fd2fff-f53b-4c7a-81ec-7aaf24984c97"
   },
   "outputs": [],
   "source": [
    "seed_text = \"AITA for\"\n",
    "next_words = 50\n",
    "\n",
    "generated_title, generated_body = generate_text(seed_text, next_words, model, max_sequence_len, input_len, tokenizer, total_words)\n",
    "print(generated_title)\n",
    "print(generated_body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac44d52-b96c-49df-be06-214f94f33a0e",
   "metadata": {
    "id": "4ac44d52-b96c-49df-be06-214f94f33a0e"
   },
   "outputs": [],
   "source": [
    "# # Modify the model architecture for text generation\n",
    "# input_seq = Input(shape=(None,))\n",
    "# embedding_layer = Embedding(total_words, 100)\n",
    "# emb_seq = embedding_layer(input_seq)\n",
    "# lstm = LSTM(50, return_sequences=True)(emb_seq)\n",
    "# output = TimeDistributed(Dense(total_words, activation='softmax'))(lstm)\n",
    "# model = Model(inputs=input_seq, outputs=output)\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5701b1a-784e-4033-8007-f9967e5154c3",
   "metadata": {
    "id": "e5701b1a-784e-4033-8007-f9967e5154c3"
   },
   "outputs": [],
   "source": [
    "# model.fit(train_data, epochs=10, steps_per_epoch=len(X_title_train) // batch_size, validation_data=([X_title_test, X_body_test], y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5531e8-7851-4c63-8146-33a80081651c",
   "metadata": {
    "id": "ee5531e8-7851-4c63-8146-33a80081651c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# #     def compute_reward(self, sequence):\n",
    "# #         # Assign weights to each component\n",
    "# #         w1, w2, w3, w4 = 0.25, 0.25, 0.25, 0.25\n",
    "\n",
    "# #         # Sentiment analysis reward\n",
    "# #         sentiment_score = sia.polarity_scores(sequence)\n",
    "# #         sentiment_reward = 0\n",
    "# #         if sentiment_score['pos'] > 0 and sentiment_score['neg'] > 0:\n",
    "# #             sentiment_reward = 1\n",
    "\n",
    "# #         # Topical coherence reward\n",
    "# #         coherence_reward = compute_topic_coherence(sequence)\n",
    "\n",
    "# #         # Post structure reward\n",
    "# #         post_structure_reward = compute_post_structure_reward(sequence)\n",
    "\n",
    "# #         # Grammar and fluency reward\n",
    "# #         grammar_errors = grammar_check(sequence)\n",
    "# #         grammar_reward = 0\n",
    "# #         if grammar_errors == 0:\n",
    "# #             grammar_reward = 1\n",
    "\n",
    "# #         # Combine the weighted scores to obtain the final reward\n",
    "# #         reward = (w1 * sentiment_reward +\n",
    "# #                   w2 * coherence_reward +\n",
    "# #                   w3 * post_structure_reward +\n",
    "# #                   w4 * grammar_reward)\n",
    "\n",
    "# #         return reward\n",
    "\n",
    "# import gym\n",
    "# from gym import spaces\n",
    "# import numpy as np\n",
    "# from typing import Dict\n",
    "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# import language_tool_python\n",
    "\n",
    "# sia = SentimentIntensityAnalyzer()\n",
    "# tool = language_tool_python.LanguageTool('en-US')\n",
    "# vocab_size = 150000\n",
    "# max_episode_length = 50  # or any suitable value based on your requirements\n",
    "\n",
    "# def grammar_check(sequence):\n",
    "#     matches = tool.check(sequence)\n",
    "#     return len(matches)\n",
    "\n",
    "# class TextGenerationEnv(gym.Env):\n",
    "#     def __init__(self, model, tokenizer, max_title_len, max_body_len, max_episode_length):\n",
    "#         super(TextGenerationEnv, self).__init__()\n",
    "#         self.max_sequence_len = max_title_len + max_body_len\n",
    "#         self.model = model\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_title_len = max_title_len\n",
    "#         self.max_body_len = max_body_len\n",
    "#         self.max_episode_length = max_episode_length\n",
    "\n",
    "#         # self.action_space = spaces.Dict({\n",
    "#         #     'title': spaces.MultiDiscrete([vocab_size] * max_title_len),\n",
    "#         #     'body': spaces.MultiDiscrete([vocab_size] * max_body_len)\n",
    "#         # })\n",
    "#         # self.observation_space = spaces.Dict({\n",
    "#         #     'title': spaces.MultiDiscrete([vocab_size] * max_title_len),\n",
    "#         #     'body': spaces.MultiDiscrete([vocab_size] * max_body_len)\n",
    "#         # })\n",
    "# #         self.action_space = spaces.Dict({\n",
    "# #             'title': spaces.Discrete(vocab_size),\n",
    "# #             'body': spaces.Discrete(vocab_size)\n",
    "# #         })\n",
    "\n",
    "# #         self.observation_space = spaces.Dict({\n",
    "# #             'title': spaces.Box(low=0, high=vocab_size, shape=(max_title_len,), dtype=np.int32),\n",
    "# #             'body': spaces.Box(low=0, high=vocab_size, shape=(max_body_len,), dtype=np.int32)\n",
    "# #         })\n",
    "\n",
    "\n",
    "#         self.action_space = spaces.Dict({\n",
    "#             'type': spaces.Discrete(2),  # 0 for title, 1 for body\n",
    "#             'token': spaces.Discrete(vocab_size)\n",
    "#         })\n",
    "\n",
    "#         self.observation_space = spaces.Dict({\n",
    "#             'title': spaces.MultiDiscrete([vocab_size] * max_title_len),\n",
    "#             'body': spaces.MultiDiscrete([vocab_size] * max_body_len)\n",
    "#         })\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.current_title = [0] * self.max_title_len\n",
    "#         self.current_body = [0] * self.max_body_len\n",
    "#         self.current_step = 0\n",
    "#         return {\n",
    "#             'title': self.current_title,\n",
    "#             'body': self.current_body\n",
    "#         }\n",
    "\n",
    "# #     def step(self, action: Dict[str, np.ndarray]):\n",
    "# #         for key in ['title', 'body']:\n",
    "# #             sequence = self.current_title if key == 'title' else self.current_body\n",
    "# #             max_len = self.max_title_len if key == 'title' else self.max_body_len\n",
    "\n",
    "# #             sequence[:-max_len] = sequence[max_len:]\n",
    "# #             sequence[-max_len:] = action[key]\n",
    "\n",
    "# #             self.current_step += 1\n",
    "\n",
    "\n",
    "\n",
    "# #     def step(self, action: Dict[str, np.ndarray]):\n",
    "# #         for key in ['title', 'body']:\n",
    "# #             sequence = self.current_title if key == 'title' else self.current_body\n",
    "# #             max_len = self.max_title_len if key == 'title' else self.max_body_len\n",
    "\n",
    "# #             sequence[:-1] = sequence[1:]\n",
    "# #             sequence[-1] = action[key]\n",
    "\n",
    "# #         self.current_step += 1\n",
    "#     def step(self, action: Dict[str, np.ndarray]):\n",
    "#         sequence_type = action['type']\n",
    "#         token = action['token']\n",
    "#         sequence = self.current_title if sequence_type == 0 else self.current_body\n",
    "#         max_len = self.max_title_len if sequence_type == 0 else self.max_body_len\n",
    "\n",
    "#         sequence[:-1] = sequence[1:]\n",
    "#         sequence[-1] = token\n",
    "\n",
    "#         self.current_step += 1\n",
    "\n",
    "#         # Compute the reward based on the generated text (use a custom reward function)\n",
    "#         title_reward = self.compute_title_reward(self.current_title)\n",
    "#         body_reward = self.compute_body_reward(self.current_body)\n",
    "\n",
    "#         # Combine the weighted scores to obtain the final reward\n",
    "#         w_title, w_body = 0.5, 0.5\n",
    "#         reward = (w_title * title_reward + w_body * body_reward)\n",
    "#         reward /= (w_title + w_body)\n",
    "\n",
    "#         done = self.current_step >= self.max_episode_length\n",
    "\n",
    "#         return {\n",
    "#             'title': self.current_title,\n",
    "#             'body': self.current_body\n",
    "#         }, reward, done, {}\n",
    "#  # body = tokenizer.sequences_to_texts([body_sequence])[0]\n",
    "#     # # Sentiment analysis reward\n",
    "#     # sentiment_score = sia.polarity_scores(body)\n",
    "#     # reward = 0\n",
    "#     # if sentiment_score['pos'] > 0 and sentiment_score['neg'] > 0:\n",
    "#     #     reward += 1\n",
    "\n",
    "\n",
    "\n",
    "# def compute_title_reward(self, title_sequence):\n",
    "#     title = self.tokenizer.sequences_to_texts([title_sequence])[0]\n",
    "#     title_reward = 0\n",
    "#     title_keywords = ['aita', 'wibta', 'am i the']\n",
    "#     title_present = any(keyword.lower() in title.lower() for keyword in title_keywords)\n",
    "#     if title_present:\n",
    "#         title_reward += 1\n",
    "\n",
    "#     grammar_errors = grammar_check(title)\n",
    "#     grammar_reward = 0\n",
    "#     if grammar_errors == 0:\n",
    "#         grammar_reward += 1\n",
    "\n",
    "#     # Combine the weighted scores to obtain the final reward\n",
    "#     w_title, w_grammar = 0.5, 0.5\n",
    "#     reward = (w_title * title_reward + w_grammar * grammar_reward)\n",
    "#     reward /= (w_title + w_grammar)\n",
    "\n",
    "#     return reward\n",
    "\n",
    "# def compute_body_reward(self, body_sequence):\n",
    "#     body = self.tokenizer.sequences_to_texts([body_sequence])[0]\n",
    "\n",
    "#     # Sentiment analysis reward\n",
    "#     sentiment_score = sia.polarity_scores(body)\n",
    "#     sentiment_reward = 1 if abs(sentiment_score['compound']) >= 0.75 else 0\n",
    "\n",
    "#     # Topical coherence reward\n",
    "#     coherence_reward = self.compute_topic_coherence(body)\n",
    "\n",
    "#     # Post structure reward\n",
    "#     situation_keywords = ['situation', 'happened', 'issue']\n",
    "#     action_keywords = ['action', 'did', 'took']\n",
    "#     justifiable_keywords = ['justifiable', 'wrong', 'right']\n",
    "#     situation_present = any(keyword.lower() in body.lower() for keyword in situation_keywords)\n",
    "#     action_present = any(keyword.lower() in body.lower() for keyword in action_keywords)\n",
    "#     justifiable_present = any(keyword.lower() in body.lower() for keyword in justifiable_keywords)\n",
    "#     structure_reward = sum([situation_present, action_present, justifiable_present])\n",
    "\n",
    "#     # Grammar and fluency reward\n",
    "#     grammar_errors = grammar_check(body)\n",
    "#     grammar_reward = 1 if grammar_errors == 0 else 0\n",
    "\n",
    "#     # Combine the weighted scores to obtain the final reward\n",
    "#     w_sentiment, w_coherence, w_structure, w_grammar = 1, 1, 3, 1\n",
    "#     reward = (w_sentiment * sentiment_reward +\n",
    "#               w_coherence * coherence_reward +\n",
    "#               w_structure * structure_reward +\n",
    "#               w_grammar * grammar_reward)\n",
    "#     reward /= (w_sentiment + w_coherence + w_structure + w_grammar)\n",
    "\n",
    "#     return reward\n",
    "\n",
    "#    def compute_topic_coherence(self):\n",
    "#     # Preprocess the title and body\n",
    "#     title = self.tokenizer.sequences_to_texts([self.current_title])[0]\n",
    "#     body = self.tokenizer.sequences_to_texts([self.current_body])[0]\n",
    "#     tokenized_title = preprocess_text_lda(title)\n",
    "#     tokenized_body = preprocess_text_lda(body)\n",
    "\n",
    "#     # Combine the tokenized title and body\n",
    "#     tokenized_sequence = tokenized_title + tokenized_body\n",
    "\n",
    "#     # Create a bag-of-words representation\n",
    "#     bow_sequence = dictionary.doc2bow(tokenized_sequence)\n",
    "\n",
    "#     # Get topic distribution for the generated sequence\n",
    "#     topic_dist = lda_model.get_document_topics(bow_sequence)\n",
    "\n",
    "#     # Compute coherence based on the highest probability topic\n",
    "#     coherence = max([prob for _, prob in topic_dist])\n",
    "\n",
    "#     return coherence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633b563-77ff-4f88-b4de-1549164e07b0",
   "metadata": {
    "id": "e633b563-77ff-4f88-b4de-1549164e07b0"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import language_tool_python\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "#sentiment analysis\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "#max_episode_length = 50  #max_episode_length is a term commonly used in reinforcement learning, referring to the maximum number of steps (or actions) allowed in each episode of the learning environment.\n",
    "max_episode_length = input_len\n",
    "\n",
    "#similarity between title and body\n",
    "def compute_cosine_similarity(title, body):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([title, body])\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "    return similarity\n",
    "\n",
    "def grammar_check(sequence):\n",
    "    matches = tool.check(sequence)\n",
    "    return len(matches)\n",
    "\n",
    "def preprocess_for_ppo(generated_text):\n",
    "    tokenized_text = preprocess_text_lda(generated_text)\n",
    "    return tokenized_text\n",
    "\n",
    "\n",
    "\n",
    "class TextGenerationEnv(gym.Env):\n",
    "    def __init__(self, model, tokenizer, max_title_len, max_body_len, max_episode_length, preprocess_for_ppo):\n",
    "        super(TextGenerationEnv, self).__init__()\n",
    "        self.preprocess_fn = preprocess_for_ppo\n",
    "        self.max_sequence_len = max_title_len + max_body_len\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_title_len = max_title_len\n",
    "        self.max_body_len = max_body_len\n",
    "        self.max_episode_length = max_episode_length\n",
    "        self.action_space = spaces.Discrete(total_words)\n",
    "        self.observation_space = spaces.Box(low=0, high=total_words - 1, shape=(max_title_len + max_body_len,), dtype=np.int32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_text = self.generate_initial_state()\n",
    "        self.current_title = self.current_text[:self.max_title_len]\n",
    "        self.current_step = 0\n",
    "        #print(\"Reset: current_text shape\", self.current_text.shape)  # Add this line\n",
    "        print(\"Reset: current_text length\", len(self.current_text))\n",
    "        return self.current_text\n",
    "\n",
    "    def generate_initial_state(self):\n",
    "        initial_title = np.random.randint(1, total_words, self.max_title_len - 1)\n",
    "        initial_body = np.random.randint(1, total_words, self.max_body_len)\n",
    "        initial_text = np.concatenate((initial_title, initial_body))\n",
    "        return initial_text\n",
    "\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        token = action\n",
    "        self.current_text[:-1] = self.current_text[1:]\n",
    "        self.current_text[-1] = token\n",
    "\n",
    "        self.current_step += 1\n",
    "        generated_text = self.tokenizer.sequences_to_texts([self.current_text])[0]\n",
    "\n",
    "        # Split the generated text into title and body\n",
    "        title_body_split = re.split(r'[.?!]', generated_text, 1)\n",
    "        title = title_body_split[0]\n",
    "        body = title_body_split[1] if len(title_body_split) > 1 else \"\"\n",
    "\n",
    "        # Compute the reward using the compute_reward function\n",
    "        reward = self.compute_reward(title, body)\n",
    "\n",
    "        input_sequences = np.expand_dims(self.current_text[:-1], axis=0)\n",
    "        target_sequences = np.expand_dims(self.current_text[1:], axis=0)\n",
    "        self.model.fit(input_sequences, target_sequences, epochs=1, verbose=0, batch_size=1)\n",
    "\n",
    "        done = self.current_step >= self.max_episode_length\n",
    "\n",
    "        return self.current_text, reward, done, {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     def compute_title_reward(self, title_sequence):\n",
    "#         title = self.tokenizer.sequences_to_texts([title_sequence])[0]\n",
    "#         title_reward = 0\n",
    "#         title_keywords = ['aita', 'wibta', 'am i the']\n",
    "#         title_present = any(keyword.lower() in title.lower() for keyword in title_keywords)\n",
    "#         if title_present:\n",
    "#             title_reward += 1\n",
    "\n",
    "#         grammar_errors = grammar_check(title)\n",
    "#         grammar_reward = 0\n",
    "#         if grammar_errors == 0:\n",
    "#             grammar_reward += 1\n",
    "\n",
    "#         # Combine the weighted scores to obtain the final reward\n",
    "#         w_title, w_grammar = 0.5, 0.5\n",
    "#         reward = (w_title * title_reward + w_grammar * grammar_reward)\n",
    "#         reward /= (w_title + w_grammar)\n",
    "\n",
    "#         return reward\n",
    "\n",
    "    def compute_title_reward(self, preprocessed_title, preprocessed_body):\n",
    "        title_reward = 0\n",
    "        title_keywords = ['aita', 'wibta', 'am i the']\n",
    "        title_present = any(keyword in preprocessed_title for keyword in title_keywords)\n",
    "        if title_present:\n",
    "            title_reward += 1\n",
    "\n",
    "        grammar_errors = grammar_check(' '.join(preprocessed_title))\n",
    "        grammar_reward = 0\n",
    "        if grammar_errors == 0:\n",
    "            grammar_reward += 1\n",
    "\n",
    "        # Cosine similarity between title and body\n",
    "        title_body_similarity = compute_cosine_similarity(' '.join(preprocessed_title), ' '.join(preprocessed_body))\n",
    "\n",
    "        # Combine the weighted scores to obtain the final reward\n",
    "        w_title, w_grammar, w_similarity = 0.4, 0.4, 0.2\n",
    "        reward = compute_weighted_score([w_title, w_grammar, w_similarity], [title_reward, grammar_reward, title_body_similarity])\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def compute_body_reward(self, preprocessed_title, preprocessed_body):\n",
    "        body_reward = 0\n",
    "        body = ' '.join(preprocessed_body)\n",
    "\n",
    "        # Sentiment analysis reward\n",
    "        sentiment_score = sia.polarity_scores(body)\n",
    "        sentiment_reward = sentiment_score['compound']\n",
    "\n",
    "        # Topical coherence reward\n",
    "        coherence_reward = self.compute_topic_coherence(body)\n",
    "\n",
    "        # Post structure reward\n",
    "        situation_keywords = ['situation', 'happened', 'issue']\n",
    "        action_keywords = ['action', 'did', 'took']\n",
    "        justifiable_keywords = ['justifiable', 'wrong', 'right']\n",
    "        situation_present = any(keyword in preprocessed_body for keyword in situation_keywords)\n",
    "        action_present = any(keyword in preprocessed_body for keyword in action_keywords)\n",
    "        justifiable_present = any(keyword in preprocessed_body for keyword in justifiable_keywords)\n",
    "        w_situation, w_action, w_justifiable = 2, 1, 1\n",
    "        structure_reward = (w_situation * situation_present +\n",
    "                            w_action * action_present +\n",
    "                            w_justifiable * justifiable_present)\n",
    "        structure_reward /= (w_situation + w_action + w_justifiable)\n",
    "\n",
    "        # Grammar and fluency reward\n",
    "        grammar_errors = grammar_check(body)\n",
    "        grammar_reward = max(0, 1 - grammar_errors / len(preprocessed_body))\n",
    "\n",
    "        # Cosine similarity between title and body\n",
    "        title_body_similarity = compute_cosine_similarity(' '.join(preprocessed_title), body)\n",
    "\n",
    "        # Combine the weighted scores to obtain the final reward\n",
    "        w_sentiment, w_coherence, w_structure, w_grammar, w_similarity = 1, 1, 3, 1, 1\n",
    "        reward = compute_weighted_score([w_sentiment, w_coherence, w_structure, w_grammar, w_similarity], [sentiment_reward, coherence_reward, structure_reward, grammar_reward, title_body_similarity])\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def compute_topic_coherence(self, body):\n",
    "        # Preprocess the title and body\n",
    "        title = self.tokenizer.sequences_to_texts([self.current_title])[0]\n",
    "        tokenized_title = preprocess_text_lda(title)\n",
    "        tokenized_body = preprocess_text_lda(body)\n",
    "\n",
    "        # Combine the tokenized title and body\n",
    "        tokenized_sequence = tokenized_title + tokenized_body\n",
    "\n",
    "        # Create a bag-of-words representation\n",
    "        bow_sequence = dictionary.doc2bow(tokenized_sequence)\n",
    "\n",
    "        # Get topic distribution for the generated sequence\n",
    "        topic_dist = lda_model.get_document_topics(bow_sequence)\n",
    "\n",
    "        # Get the most probable topic\n",
    "        most_probable_topic = max(topic_dist, key=lambda x: x[1])[0]\n",
    "\n",
    "        # Compute coherence using Gensim's CoherenceModel\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=[tokenized_sequence], dictionary=dictionary, coherence='c_v')\n",
    "        coherence = coherence_model_lda.get_coherence_per_topic()[most_probable_topic]\n",
    "\n",
    "        return coherence\n",
    "\n",
    "\n",
    "    def compute_reward(self, title, body):\n",
    "        # Preprocess the title and body\n",
    "        preprocessed_title = preprocess_text_lda(title)\n",
    "        preprocessed_body = preprocess_text_lda(body)\n",
    "\n",
    "        # Compute rewards for the title and body\n",
    "        title_reward = self.compute_title_reward(preprocessed_title, preprocessed_body)\n",
    "        body_reward = self.compute_body_reward(preprocessed_title, preprocessed_body)\n",
    "\n",
    "        # Combine the title and body rewards with appropriate weights\n",
    "        w_title, w_body = 0.5, 0.5\n",
    "        reward = (w_title * title_reward + w_body * body_reward)\n",
    "        reward /= (w_title + w_body)\n",
    "\n",
    "        return reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f452f-7c04-454c-b9a0-cd70f690dd6b",
   "metadata": {
    "id": "431f452f-7c04-454c-b9a0-cd70f690dd6b"
   },
   "outputs": [],
   "source": [
    "#     def compute_reward(self, sequence):\n",
    "#         reward = 0\n",
    "#         # Compute cosine similarity between consecutive words in the sequence\n",
    "#         for i in range(len(sequence) - 1):\n",
    "#             word1 = self.tokenizer.index_word.get(sequence[i], None)\n",
    "#             word2 = self.tokenizer.index_word.get(sequence[i+1], None)\n",
    "\n",
    "#             if word1 is not None and word2 is not None:\n",
    "#                 embedding1 = embeddings_dictionary.get(word1, None)\n",
    "#                 embedding2 = embeddings_dictionary.get(word2, None)\n",
    "\n",
    "#                 if embedding1 is not None and embedding2 is not None:\n",
    "#                     cosine_similarity = dot(embedding1, embedding2) / (norm(embedding1) * norm(embedding2))\n",
    "#                     reward += cosine_similarity\n",
    "\n",
    "#         return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1422c42-ba9b-4f10-acf8-63d385286b91",
   "metadata": {
    "id": "b1422c42-ba9b-4f10-acf8-63d385286b91"
   },
   "source": [
    "This compute_reward function computes the reward based on the cosine similarity between consecutive words in the generated sequence. It sums up the cosine similarities for all consecutive word pairs, encouraging the model to generate text with similar words in close proximity. Note that this is just an example reward function, and you may need to experiment with different reward functions depending on your specific requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3653c6d-8f18-4f4a-81fe-ab09b18dc19b",
   "metadata": {
    "id": "c3653c6d-8f18-4f4a-81fe-ab09b18dc19b"
   },
   "source": [
    "The cosine similarity-based reward function I provided is more focused on generating sentences that make sense by considering the similarity between consecutive words. For generating AITA posts, you might want to use a more complex reward function that takes into account additional factors.\n",
    "\n",
    "Here are a few ideas for a custom reward function tailored to AITA posts:\n",
    "\n",
    "Sentiment analysis: AITA posts often involve a mix of emotions. You can use a pre-trained sentiment analysis model (such as VADER or TextBlob) to analyze the sentiment of the generated text and reward the model based on the presence of a mix of emotions.\n",
    "\n",
    "Topical coherence: You can use topic modeling techniques (such as LDA) to ensure that the generated post is coherent and focused on a specific topic. Reward the model based on the coherence score.\n",
    "\n",
    "Post structure: AITA posts usually follow a particular structure: they present a situation, describe the actions taken, and ask if the actions were justifiable. You can create a custom reward function that evaluates if the generated text follows this structure.\n",
    "\n",
    "Grammar and fluency: Reward the model based on the grammatical correctness and fluency of the generated text. You can use tools like the LanguageTool library for grammar checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a013f-9e01-4c46-83bd-6a3315d7a0d4",
   "metadata": {
    "id": "5d1a013f-9e01-4c46-83bd-6a3315d7a0d4"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Topical coherence\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Preprocess the text\n",
    "def remove_tags(text):\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def preprocess_text_lda(text):\n",
    "    corpus = []\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = remove_tags(text)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    text = pattern.sub('', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenized_text = word_tokenize(text)\n",
    "\n",
    "    return tokenized_text\n",
    "\n",
    "\n",
    "# Create a list of tokenized titles and bodies\n",
    "tokenized_titles = [preprocess_text_lda(text) for text in df['title']]\n",
    "tokenized_bodies = [preprocess_text_lda(text) for text in df['body']]\n",
    "\n",
    "# Combine the tokenized titles and bodies into a single list of documents\n",
    "documents = [title + body for title, body in zip(tokenized_titles, tokenized_bodies)]\n",
    "\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = Dictionary(documents)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# Create a bag-of-words representation of the documents\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "\n",
    "# Train the LDA model with increased passes and iterations\n",
    "num_topics = 10  # You can choose the number of topics based on your dataset\n",
    "passes = 20  # Increase the number of passes\n",
    "iterations = 200  # Increase the number of iterations\n",
    "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, random_state=42, passes=passes, iterations=iterations)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Your custom reward function looks good for checking the post structure based on the presence of specific keywords. However, if you're using a combined dataset of title and body,\n",
    "#you might want to modify the function to ensure the generated text has a distinct title and body that follow the structure of AITA posts.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70011d88-b31c-4762-af97-594174835a1e",
   "metadata": {
    "id": "70011d88-b31c-4762-af97-594174835a1e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ba8be-4c99-42fb-a5c3-b63b48af1b55",
   "metadata": {
    "id": "2b5ba8be-4c99-42fb-a5c3-b63b48af1b55"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# Create a training environment\n",
    "train_env = TextGenerationEnv(model, tokenizer, max_title_len, max_body_len, max_episode_length, preprocess_for_ppo)\n",
    "train_env = DummyVecEnv([lambda: train_env])\n",
    "\n",
    "# Train the PPO agent\n",
    "ppo_agent = PPO(\"MlpPolicy\", train_env, verbose=1)\n",
    "\n",
    "# Create an evaluation environment\n",
    "eval_env = TextGenerationEnv(model, tokenizer, max_title_len, max_body_len, max_episode_length, preprocess_for_ppo)\n",
    "eval_env = DummyVecEnv([lambda: eval_env])\n",
    "\n",
    "# Use the same PPO agent for evaluation\n",
    "agent = ppo_agent\n",
    "\n",
    "# Create the SaveBestModelCallback instance\n",
    "callback = SaveBestModelCallback(save_path, eval_env, check_freq=1000)\n",
    "\n",
    "# Train the agent with the callback\n",
    "agent.learn(total_timesteps=timesteps, callback=callback)\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./models/',\n",
    "                             log_path='./logs/', eval_freq=10000,\n",
    "                             deterministic=True, render=False,\n",
    "                             save_best=True)\n",
    "\n",
    "ppo_agent.learn(total_timesteps=50000, callback=eval_callback)\n",
    "\n",
    "# Load the best model\n",
    "best_agent = PPO.load(save_path)\n",
    "\n",
    "# Generate text using the trained agent\n",
    "obs = eval_env.reset()\n",
    "done = False\n",
    "generated_text = []\n",
    "\n",
    "while not done:\n",
    "    action, _states = best_agent.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    generated_text.append(tokenizer.sequences_to_texts([obs])[0])\n",
    "\n",
    "# Join the generated text and split it into title and body using the separator token\n",
    "generated_text = \" \".join(generated_text)\n",
    "generated_title, generated_body = generated_text.split(separator_token)\n",
    "\n",
    "print(\"Generated title:\", generated_title)\n",
    "print(\"Generated body:\", generated_body)\n",
    "\n",
    "# Evaluate the agent\n",
    "total_rewards = []\n",
    "for _ in range(35):  # You can adjust the number of evaluation episodes\n",
    "    obs = eval_env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        action, _ = best_agent.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = eval_env.step(action)\n",
    "        episode_reward += reward\n",
    "    total_rewards.append(episode_reward)\n",
    "\n",
    "avg_reward = np.mean(total_rewards)\n",
    "print(f\"Average reward for the best model: {avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c4607-860d-4acd-b0bd-e95200d9557c",
   "metadata": {
    "id": "1c4c4607-860d-4acd-b0bd-e95200d9557c"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class SaveBestModelCallback(BaseCallback):\n",
    "    def __init__(self, save_path, eval_env, check_freq, verbose=0):\n",
    "        super(SaveBestModelCallback, self).__init__(verbose)\n",
    "        self.save_path = save_path\n",
    "        self.eval_env = eval_env\n",
    "        self.check_freq = check_freq\n",
    "        self.best_reward = -float(\"inf\")\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            # Evaluate the current model\n",
    "            total_rewards = []\n",
    "            for _ in range(5):  # You can adjust the number of evaluation episodes\n",
    "                obs = self.eval_env.reset()\n",
    "                done = False\n",
    "                episode_reward = 0\n",
    "                while not done:\n",
    "                    action, _ = self.model.predict(obs, deterministic=True)\n",
    "                    obs, reward, done, _ = self.eval_env.step(action)\n",
    "                    episode_reward += reward\n",
    "                total_rewards.append(episode_reward)\n",
    "\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "            print(f\"Average reward after {self.n_calls} steps: {avg_reward}\")  # Add this line to print the average reward\n",
    "            if avg_reward > self.best_reward:\n",
    "                self.best_reward = avg_reward\n",
    "                self.model.save(self.save_path)\n",
    "                if self.verbose:\n",
    "                    print(f\"New best model with reward {avg_reward}, model saved.\")\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "save_path = \"best_model\"\n",
    "eval_env = TextGenerationEnv(model, tokenizer, max_sequence_len, max_episode_length)\n",
    "eval_env = DummyVecEnv([lambda: eval_env])\n",
    "callback = SaveBestModelCallback(save_path, eval_env, check_freq=5000)\n",
    "\n",
    "\n",
    "for i, sample in enumerate(generated_samples):\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(sample)\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d872c1-5e1c-4632-b3b3-b03e94b62300",
   "metadata": {
    "id": "72d872c1-5e1c-4632-b3b3-b03e94b62300"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ba89d-45dc-440d-a7b2-4c6f6f677066",
   "metadata": {
    "id": "7b1ba89d-45dc-440d-a7b2-4c6f6f677066"
   },
   "outputs": [],
   "source": [
    "#use gpt 2\n",
    "# from transformers import GPT2LMHeadModel\n",
    "\n",
    "# teacher_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# from transformers import GPT2Config\n",
    "\n",
    "# student_config = GPT2Config.from_pretrained(\"gpt2\", n_layer=6, n_embd=768 // 2, n_head=12, n_positions=1024)\n",
    "# student_model = GPT2LMHeadModel(student_config)\n",
    "\n",
    "# import torch\n",
    "\n",
    "# def distillation_loss(student_logits, teacher_logits, temperature=2.0):\n",
    "#     student_probs = torch.nn.functional.log_softmax(student_logits / temperature, dim=-1)\n",
    "#     teacher_probs = torch.nn.functional.softmax(teacher_logits / temperature, dim=-1)\n",
    "#     return torch.nn.functional.kl_div(student_probs, teacher_probs, reduction=\"batchmean\")\n",
    "# from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     save_steps=10_000,\n",
    "#     save_total_limit=2,\n",
    "# )\n",
    "\n",
    "# def compute_loss(model, inputs, teacher_model):\n",
    "#     student_logits = model(**inputs).logits\n",
    "#     with torch.no_grad():\n",
    "#         teacher_logits = teacher_model(**inputs).logits\n",
    "#     return distillation_loss(student_logits, teacher_logits)\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=student_model,\n",
    "#     args=training_args,\n",
    "#     compute_loss=compute_loss,\n",
    "#     train_dataset=tokenized_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dadd8d3-282b-4269-a429-055e83ca5aaf",
   "metadata": {
    "id": "1dadd8d3-282b-4269-a429-055e83ca5aaf"
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilled_gpt2\", tokenizer=tokenizer)\n",
    "\n",
    "generated_text = generator(\"AITA for\", max_length=50, do_sample=True, temperature=0.8)\n",
    "print(generated_text[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
