{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2580aefb-e96a-4902-83b3-009af05c4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c90b7a-9fa0-4792-bd33-6976e9718940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\public\\downloads\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('X:/home/comp creativity/AITA generator/aita_clean.csv', low_memory=True)\n",
    "df = df.dropna()\n",
    "# Filter the data for each class\n",
    "class_0 = df[df['is_asshole'] == 0]\n",
    "class_1 = df[df['is_asshole'] == 1]\n",
    "# Sample an equal number of instances from each class\n",
    "sample_size = 500\n",
    "class_0_sample = class_0.sample(sample_size, random_state=42)\n",
    "class_1_sample = class_1.sample(sample_size, random_state=42)\n",
    "# Combine the sampled data and shuffle it\n",
    "df = pd.concat([class_0_sample, class_1_sample]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# Preprocess the data\n",
    "def remove_tags(text):\n",
    "    return BeautifulSoup(text, 'html.parser').get_text()\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "   # text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = remove_tags(text)\n",
    "    # Replace punctuations and numbers with a space (except for '.', ',', '!', '?')\n",
    "    text = re.sub('[^a-zA-Z.,!?]', ' ', text)\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "# Preprocess titles and bodies using the preprocess_text function\n",
    "preprocessed_titles = [preprocess_text(text) for text in df['title']]\n",
    "preprocessed_bodies = [preprocess_text(text) for text in df['body']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38479e1c-0bd0-489c-a321-c9e53bd02a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_and_pos(text):\n",
    "    doc = nlp(text)\n",
    "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "    return named_entities, pos_tags\n",
    "\n",
    "# Define a grammar_check function\n",
    "def grammar_check(text):\n",
    "    tb = TextBlob(text)\n",
    "    return tb.edit_distance(tb.correct())\n",
    "def compute_body_reward(preprocessed_body):\n",
    "    body_reward = 0\n",
    "    body = preprocessed_body\n",
    "    sentiment_score = sia.polarity_scores(body)\n",
    "    sentiment_reward = 1 if abs(sentiment_score['compound']) >= 0.75 else 0\n",
    "\n",
    "    named_entities, pos_tags = extract_entities_and_pos(preprocessed_body)\n",
    "\n",
    "    # Reward based on the presence of specific POS tags or entities\n",
    "    nlp_reward = 0\n",
    "    if any([tag[1] == 'VERB' for tag in pos_tags]):\n",
    "        nlp_reward += 1\n",
    "    if any([ent[1] == 'PERSON' for ent in named_entities]):\n",
    "        nlp_reward += 1\n",
    "\n",
    "    grammar_errors = grammar_check(body)\n",
    "    grammar_reward = 1 if grammar_errors == 0 else 0\n",
    "\n",
    "    w_sentiment, w_nlp, w_grammar = 1, 2, 1\n",
    "    reward = (w_sentiment * sentiment_reward +\n",
    "              w_nlp * nlp_reward +\n",
    "              w_grammar * grammar_reward)\n",
    "    reward /= (w_sentiment + w_nlp + w_grammar)\n",
    "\n",
    "    return reward\n",
    "\n",
    "    \n",
    "def compute_body_reward(preprocessed_body):\n",
    "    body_reward = 0\n",
    "    body = preprocessed_body\n",
    "    sentiment_score = sia.polarity_scores(body)\n",
    "    sentiment_reward = 1 if abs(sentiment_score['compound']) >= 0.75 else 0\n",
    "    situation_keywords = ['situation', 'happened', 'issue']\n",
    "    action_keywords = ['action', 'did', 'took']\n",
    "    justifiable_keywords = ['justifiable', 'wrong', 'right']\n",
    "    situation_present = any(keyword in token.text for keyword in situation_keywords for token in nlp(preprocessed_body))\n",
    "    action_present = any(keyword in token.text for keyword in action_keywords for token in nlp(preprocessed_body))\n",
    "    justifiable_present = any(keyword in token.text for keyword in justifiable_keywords for token in nlp(preprocessed_body))\n",
    "    structure_reward = sum([situation_present, action_present, justifiable_present])\n",
    "    grammar_errors = grammar_check(body)\n",
    "    grammar_reward = 1 if grammar_errors == 0 else 0\n",
    "    w_sentiment, w_structure, w_grammar = 1, 3, 1\n",
    "    reward = (w_sentiment * sentiment_reward +\n",
    "              w_structure * structure_reward +\n",
    "              w_grammar * grammar_reward)\n",
    "    reward /= (w_sentiment + w_structure + w_grammar)\n",
    "    return reward\n",
    "def compute_reward( generated_title, generated_body):\n",
    "    # Preprocess the title and body\n",
    "    preprocessed_title = preprocess_text(generated_title)\n",
    "    preprocessed_body = preprocess_text(generated_body)\n",
    "    # Compute rewards for the title and body\n",
    "    title_reward = compute_title_reward(preprocessed_title)\n",
    "    body_reward = compute_body_reward(preprocessed_body)\n",
    "    # Combine the title and body rewards with appropriate weights\n",
    "    w_title, w_body = 0.5, 0.5\n",
    "    reward = (w_title * title_reward + w_body * body_reward)\n",
    "    reward /= (w_title + w_body)\n",
    "    return reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c422f08-dd49-4456-9b6f-965613174e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\public\\downloads\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 1:11:09, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.062900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=3.46015009765625, metrics={'train_runtime': 4277.4676, 'train_samples_per_second': 1.169, 'train_steps_per_second': 0.292, 'total_flos': 326615040000000.0, 'train_loss': 3.46015009765625, 'epoch': 5.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# Set the padding token to the end-of-sequence (eos) token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Update the tokenizer with the separator token\n",
    "separator_token = \"<SEP>\"\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [separator_token]})\n",
    "# Update the model with the new vocabulary size\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# Tokenize the dataset\n",
    "texts = [title + separator_token + body for title, body in zip(preprocessed_titles, preprocessed_bodies)]\n",
    "tokenized_texts = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, block_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_text = self.tokenizer(self.texts[idx], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=self.block_size)\n",
    "        input_ids = tokenized_text[\"input_ids\"].squeeze()\n",
    "        attention_mask = tokenized_text[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "# Prepare the dataset\n",
    "dataset = CustomTextDataset(tokenizer=tokenizer, texts=texts, block_size=128)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "# Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-reddit\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dee79f3-5129-439a-b5f8-82018e083fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# import torch\n",
    "# from transformers import DistilBertForSequenceClassification\n",
    "# from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# X = preprocessed_bodies\n",
    "# y = df['is_asshole'].values\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train_encodings = tokenizer(X_train, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# val_encodings = tokenizer(X_val, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "\n",
    "# class AITADataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, encodings, labels):\n",
    "#         self.encodings = encodings\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "#         item['labels'] = torch.tensor(self.labels[idx])\n",
    "#         return item\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "# train_dataset = AITADataset(train_encodings, y_train)\n",
    "# val_dataset = AITADataset(val_encodings, y_val)\n",
    "\n",
    "\n",
    "# S_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./sentiment_model',\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     logging_dir='./logs',\n",
    "#     logging_steps=100,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=S_model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e3a459-aa3e-40bc-b153-e65785433ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=150\n",
    "class TextProcessor:\n",
    "    def __init__(self, ending_patterns=None):\n",
    "        self.ending_patterns = ending_patterns if ending_patterns else ['?']\n",
    "\n",
    "    def check_ending(self, text):\n",
    "        return any(text.strip().endswith(pattern) for pattern in self.ending_patterns)\n",
    "\n",
    "    def add_proper_ending(self, text):\n",
    "        return text.strip() + self.ending_patterns[0]\n",
    "\n",
    "    def generate_text(self, prompt, model, tokenizer, max_length=max_length, max_attempts=10, temperature=0.7):\n",
    "        # Tokenize the prompt\n",
    "        tokenized_prompt = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n",
    "        input_ids = tokenized_prompt[\"input_ids\"]\n",
    "        attention_mask = tokenized_prompt[\"attention_mask\"]\n",
    "\n",
    "        for _ in range(max_attempts):\n",
    "            # Generate text\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                no_repeat_ngram_size=2,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=temperature,  # Pass the temperature parameter\n",
    "                attention_mask=attention_mask,  # Pass the attention mask\n",
    "            )\n",
    "\n",
    "            # Decode the generated text\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "            if self.check_ending(generated_text):\n",
    "                return generated_text\n",
    "\n",
    "        return self.add_proper_ending(generated_text)  # Add a proper ending if not found within max_attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a68e3b1-4ccb-4e96-bbd6-8d39fe689118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \" AITA for \"\n",
    "# generated_text = generate_text(prompt, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b552b04-59cc-465a-a9d5-c2d5a519c962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa6f8081-c618-494b-bbb5-9521ffc65cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = TextProcessor(ending_patterns=['.', '?', '!'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd47267c-2c6b-48ac-a088-cf9353ef1fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\users\\public\\downloads\\Anaconda3\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\da476\\AppData\\Local\\Temp\\ipykernel_19164\\4017118423.py\", line 25, in classify\n",
      "    input_text_sequence = word_tokenizer.texts_to_sequences([input_text_preprocessed])\n",
      "NameError: name 'word_tokenizer' is not defined\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk, filedialog, messagebox\n",
    "\n",
    "def generate():\n",
    "    prompt = prompt_entry.get()\n",
    "    if not prompt:\n",
    "        messagebox.showerror(\"Error\", \"Please enter a prompt\")\n",
    "        return\n",
    "\n",
    "    temperature = temperature_var.get()\n",
    "    try:\n",
    "        generated_text = text_processor.generate_text(prompt, model, tokenizer, max_length=max_length, max_attempts=10, temperature=float(temperature))\n",
    "        result_text.delete(1.0, tk.END)\n",
    "        result_text.insert(tk.END, generated_text)\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred: {e}\")\n",
    "\n",
    "def classify():\n",
    "    generated_text = result_text.get(1.0, tk.END).strip()\n",
    "    if not generated_text:\n",
    "        messagebox.showerror(\"Error\", \"Please generate text before classifying\")\n",
    "        return\n",
    "\n",
    "    input_text_preprocessed = preprocess_text(generated_text)\n",
    "    input_text_sequence = word_tokenizer.texts_to_sequences([input_text_preprocessed])\n",
    "    input_text_padded = pad_sequences(input_text_sequence, padding='post', maxlen=maxlen)\n",
    "\n",
    "    prediction = lstm_model.predict(input_text_padded)\n",
    "    assholish_percentage = round(prediction[0][0] * 100, 2)\n",
    "    not_assholish_percentage = round((1 - prediction[0][0]) * 100, 2)\n",
    "\n",
    "    messagebox.showinfo(\"Classification\", f\"This post is {assholish_percentage}% assholish and {not_assholish_percentage}% not assholish\")\n",
    "\n",
    "def generate_random():\n",
    "    prompt = \" \"\n",
    "    try:\n",
    "        generated_text = generate_text(prompt, model, tokenizer)\n",
    "        result_text.delete(1.0, tk.END)\n",
    "        result_text.insert(tk.END, generated_text)\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred: {e}\")\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"GPT Reddit\")\n",
    "\n",
    "# Create and position the UI elements\n",
    "prompt_label = ttk.Label(root, text=\"Enter the prompt:\")\n",
    "prompt_label.grid(row=0, column=0, padx=(10, 0), pady=(10, 5), sticky=tk.W)\n",
    "\n",
    "prompt_entry = ttk.Entry(root, width=60)\n",
    "prompt_entry.grid(row=0, column=1, padx=(0, 10), pady=(10, 5), sticky=tk.W)\n",
    "\n",
    "options_frame = ttk.Frame(root)\n",
    "options_frame.grid(row=1, column=1, padx=(0, 10), pady=(0, 10), sticky=tk.E)\n",
    "\n",
    "temperature_label = ttk.Label(options_frame, text=\"Temperature:\")\n",
    "temperature_label.grid(row=0, column=0, padx=(0, 5), pady=(0, 10), sticky=tk.W)\n",
    "\n",
    "temperature_var = tk.StringVar(options_frame, value=\"0.6\")\n",
    "temperature_menu = ttk.OptionMenu(options_frame, temperature_var, \"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\")\n",
    "temperature_menu.grid(row=0, column=1, padx=(0, 10), pady=(0, 10), sticky=tk.W)\n",
    "\n",
    "generate_button = ttk.Button(options_frame, text=\"Generate Text\", command=generate)\n",
    "generate_button.grid(row=0, column=2, padx=(0, 5), pady=(0, 10), sticky=tk.E)\n",
    "\n",
    "generate_random_button = ttk.Button(options_frame, text=\"I'm Feeling Lucky\", command=generate_random)\n",
    "generate_random_button.grid(row=0, column=3, padx=(0, 5), pady=(0, 10), sticky=tk.E)\n",
    "\n",
    "classify_button = ttk.Button(options_frame, text=\"Classify\", command=classify)\n",
    "classify_button.grid(row=0, column=4, padx=(0, 10), pady=(0, 10), sticky=tk.E)\n",
    "\n",
    "result_label = ttk.Label(root, text=\"Generated Text:\")\n",
    "result_label.grid(row=2, column=0, padx=(10, 0), pady=(0, 10), sticky=tk.W)\n",
    "\n",
    "result_text = tk.Text(root, wrap=tk.WORD, width=80, height=20)\n",
    "result_text.grid(row=2, column=1, padx=(0, 10), pady=(0, 10), sticky=tk.W)\n",
    "\n",
    "scrollbar = ttk.Scrollbar(root, command=result_text.yview)\n",
    "scrollbar.grid(row=2, column=2, pady=(0, 10), sticky=tk.NS)\n",
    "result_text.config(yscrollcommand=scrollbar.set)\n",
    "\n",
    "# Run the main event loop\n",
    "root.mainloop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b6ec17-0717-4452-ba9a-6d357fbf268f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
